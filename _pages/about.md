---
permalink: /
excerpt: "About"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<br />

Hi, こんにちは, 你好 :-)

> Zhihang Zhong is actively looking for job opportunities!

He is a 3rd year Ph.D. student in the Department of Computer Science, the University of Tokyo (UTokyo).
His supervisor is <a href="https://scholar.google.com/citations?user=gtfbzYwAAAAJ&hl=en" target="_blank">Prof. Imari Sato</a>, and he works closely with <a href="https://scholar.google.com/citations?hl=en&user=JD-5DKcAAAAJ" target="_blank">Prof. Yinqiang Zheng</a> and <a href="https://sites.google.com/ut-vision.org/ysato/" target="_blank">Prof. Yoichi Sato</a>.
In 2020, he received his M.E. degree from UTokyo under the supervision of <a href="https://otalab.race.t.u-tokyo.ac.jp/en/jun-ota/" target="_blank">Prof. Ota Jun</a>.
In 2018, he received his B.E. degree (Mixed Honors Class) from Chu Kochen Honors College, Zhejiang University. 

Below are his current and previous internship experiences:  
- 2022.04 - present: JEM intern at Microsoft, mentored by <a href="https://scholar.google.com/citations?user=Jkss014AAAAJ&hl=en" target="_blank">Han Hu</a>, <a href="https://scholar.google.com/citations?hl=en&user=PzyvzksAAAAJ" target="_blank">Yuhui Yuan</a> and <a href="https://scholar.google.com/citations?hl=en&user=xyc52moAAAAJ" target="_blank">Ji Li</a>
- 2021.09 - 2022.03: D-CORE intern at Microsoft Research Asia (MSRA), mentored by <a href="https://scholar.google.com/citations?hl=en&user=c3PYmxUAAAAJ" target="_blank">Stephen Lin</a>, <a href="https://scholar.google.com/citations?hl=en&user=lH4zgcIAAAAJ" target="_blank">Zhirong Wu</a> and <a href="https://scholar.google.com/citations?hl=en&user=wYIe0tYAAAAJ" target="_blank">Xiao Sun</a>
- 2019.08 - 2020.08: Research intern at the Image Research Lab, Huawei Tokyo Research Center, mentored by <a href="https://scholar.google.com/citations?hl=en&user=nFb63A4AAAAJ" target="_blank">Bo Zheng</a> and Ye Gao

Zhihang's current research interests include computational photography, machine/deep learning, and human-computer interaction.

---

News
======

<iframe src="_pages/news.html" marginwidth="30" height="260" width="700" scrolling="yes" frameborder="0"></iframe>
---

# Publications
<head>
    <style>
    table,
    th,
    td {
        border: 0px solid darkgray;
    }
    </style><title></title>
</head>

## Conferences

<table style="width:100%;border:0px;border-spacing:0px;margin-right:auto;margin-left:auto;">
<tbody>
  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/bit++.gif' width="180">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Blur Interpolation Transformer for Real-World Motion from Blur</b>
      <br>
      <strong>Zhihang Zhong</strong>,
      Mingdeng Cao,
      Xiang Ji,
      Yinqiang Zheng,
      Imari Sato
      <br>
      <em>CVPR</em>, 2023
      <br>
      <a href="https://arxiv.org/abs/2211.11423" target="_blank">arXiv</a>  
    </td>
  </tr>
  
  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/VIS-WIDE.png' width="160">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Visibility Constrained Wide-band Illumination Spectrum Design for Seeing-in-the-Dark</b>
      <br>
      Muyao Niu,
      Zhuoxiao Li,
      <strong>Zhihang Zhong</strong>,
      Yinqiang Zheng
      <br>
      <em>CVPR</em>, 2023
      <br>
      arXiv
    </td>
  </tr>  
  
  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
      <img src="images/animation-from-blur.gif" width="130">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Animation from Blur: Multi-modal Blur Decomposition with Motion Guidance</b>
      <br>
      <strong>Zhihang Zhong</strong>,
      Xiao Sun, 
      Zhirong Wu,
      Yinqiang Zheng,
      Stephen Lin,
      Imari Sato
      <br>
      <em>ECCV</em>, 2022
      <br>
      <a href="https://zzh-tech.github.io/Animation-from-Blur/" target="_blank">project page</a> /
      <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7210_ECCV_2022_paper.php" target="_blank">paper</a> /
      <a href="https://arxiv.org/abs/2207.10123" target="_blank">arXiv</a> / 
      <a href="https://github.com/zzh-tech/Animation-from-Blur" target="_blank">code</a>
    </td>
  </tr> 

  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/dual-reversed-rs.gif' width="130">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Bringing Rolling Shutter Images Alive with Dual Reversed Distortion</b>
      <br>
      <strong>Zhihang Zhong</strong>,
       Mingdeng Cao,
       Xiao Sun,
       Zhirong Wu,
       Zhongyi Zhou,
       Yinqiang Zheng,
       Stephen Lin,
       Imari Sato
      <br>
      <em>ECCV</em>, 2022, <em style="color: red">Oral (top 2.7%)</em>
      <br>
      <a href="https://zzh-tech.github.io/Dual-Reversed-RS/" target="_blank">project page</a> /
      <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4547_ECCV_2022_paper.php" target="_blank">paper</a> /
      <a href="https://arxiv.org/abs/2203.06451" target="_blank">arXiv</a> / 
      <a href="https://github.com/zzh-tech/Dual-Reversed-RS" target="_blank">code</a>
    </td>
  </tr> 
    
  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/magnitude_prior.png' width="200">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Efficient Video Deblurring Guided by Motion Magnitude</b>
      <br>
      Yusheng Wang, 
      Yunfan Lu, 
      Ye Gao, 
      Lin Wang, 
      <strong>Zhihang Zhong</strong>, 
      Yinqiang Zheng, 
      Atsushi Yamashita
      <br>
      <em>ECCV</em>, 2022
      <br>
      <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5697_ECCV_2022_paper.php" target="_blank">paper</a> / 
      <a href="https://arxiv.org/abs/2207.13374" target="_blank">arXiv</a> / 
      <a href="https://github.com/sollynoay/MMP-RNN" target="_blank">code</a>
    </td>
  </tr>

  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/adaptive_warping.png' width="180">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Learning Adaptive Warping for Real-World Rolling Shutter Correction</b>
      <br>
      Mingdeng Cao, 
      <strong>Zhihang Zhong</strong>, 
      Jiahao Wang, 
      Yinqiang Zheng, 
      Yujiu Yang
      <br>
      <em>CVPR</em>, 2022
      <br>
      <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cao_Learning_Adaptive_Warping_for_Real-World_Rolling_Shutter_Correction_CVPR_2022_paper.html" target="_blank">paper</a> / 
      <a href="https://arxiv.org/abs/2204.13886" target="_blank">arXiv</a> / 
      <a href="https://github.com/ljzycmd/BSRSC" target="_blank">code</a>
    </td>
  </tr>  

  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/beam_splitter_a.png' width="150">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Towards Rolling Shutter Correction and Deblurring in Dynamic Scenes</b>
      <br>
      <strong>Zhihang Zhong</strong>,
      Yinqiang Zheng, 
      Imari Sato
      <br>
      <em>CVPR</em>, 2021
      <br>
      <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhong_Towards_Rolling_Shutter_Correction_and_Deblurring_in_Dynamic_Scenes_CVPR_2021_paper.html" target="_blank">paper</a> / 
      <a href="https://arxiv.org/abs/2104.01601" target="_blank">arXiv</a> / 
      <a href="https://github.com/zzh-tech/RSCD" target="_blank">code</a>
    </td>
  </tr>  

  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/2020_ECCV_deblur.png' width="220">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Efficient Spatio-Temporal Recurrent Neural Network for Video Deblurring</b>
      <br>
      <strong>Zhihang Zhong</strong>,
      Ye Gao,
      Yinqiang Zheng,
      Bo Zheng
      <br>
      <em>ECCV</em>, 2020, <em style="color: red">Spotlight (top 5.0%)</em>
      <br>
      <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5116_ECCV_2020_paper.php" target="_blank">paper</a> / 
      <a href="https://arxiv.org/abs/2106.16028" target="_blank">arXiv</a> / 
      <a href="https://github.com/zzh-tech/ESTRNN" target="_blank">code</a>
    </td>
  </tr>  

  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/layer.png' width="220">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Multi-attention deep recurrent neural network for nursing action evaluation using wearable sensor</b>
      <br>
      <strong>Zhihang Zhong</strong>,
      Chingszu Lin,
      Taiki Ogata,
      Jun Ota
      <br>
      <em>IUI</em>, 2020
      <br>
      <a href="https://dl.acm.org/doi/abs/10.1145/3377325.3377530" target="_blank">paper</a>
    </td>
  </tr>  
</tbody>
</table>

## Journals
<table style="width:100%;border:0px;border-spacing:0px;margin-right:auto;margin-left:auto;">
<tbody>
    <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/bsd.png' width="200">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Real-world Video Deblurring: A Benchmark Dataset and An Eﬃcient Recurrent Neural Network</b>
      <br>
      <strong>Zhihang Zhong</strong>,
      Ye Gao,
      Yinqiang Zheng,
      Bo Zheng,
      Imari Sato
      <br>
      <em>International Journal of Computer Vision (IJCV)</em>, 2022  
      <br>
      <a href="https://link.springer.com/article/10.1007/s11263-022-01705-6" target="_blank">paper</a> / 
      <a href="https://arxiv.org/abs/2106.16028" target="_blank">arXiv</a> / 
      <a href="https://github.com/zzh-tech/ESTRNN" target="_blank">code</a>
    </td>
  </tr>  
  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/zhong10-3075477-large.jpg' width="130">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Multistream Temporal Convolutional Network for Correct/Incorrect Patient Transfer Action Detection Using Body Sensor Network</b>
      <br>
      <strong>Zhihang Zhong</strong>,
      Chingszu Lin,
      Masako Kanai-Pak,
      Jukai Maeda,
      Yasuko Kitajima,
      Mitsuhiro Nakamura,
      Noriaki Kuwahara,
      Taiki Ogata,
      Jun Ota
      <br>
      <em>IEEE Internet of Things Journal (IoTJ)</em>, 2021  
      <br>
      <a href="https://ieeexplore.ieee.org/document/9415629" target="_blank">paper</a> / 
      <a href="https://github.com/zzh-tech/Continuous-Action-Detection" target="_blank">code</a>  
    </td>
  </tr>  
  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/patient_robot.png' width="240">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Development and validation of robot patient equipped with an inertial measurement unit and angular position sensors to evaluate transfer skills of nurses</b>
      <br>
      Chingszu Lin,
      Taiki Ogata,
      <strong>Zhihang Zhong</strong>,
      Masako Kanai-Pak,
      Jukai Maeda,
      Yasuko Kitajima,
      Mitsuhiro Nakamura,
      Noriaki Kuwahara,
      Jun Ota
      <br>
      <em>International Journal of Social Robotics (IJSR)</em>, 2021  
      <br>
      <a href="https://link.springer.com/article/10.1007/s12369-020-00673-6" target="_blank">paper</a>
    </td>
  </tr>
</tbody>
</table>

## Preprints
<table style="width:100%;border:0px;border-spacing:0px;margin-right:auto;margin-left:auto;">
<tbody>
    <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/clipcrop.png' width="200">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>ClipCrop: Conditioned Cropping Driven by Vision-Language Model</b>
      <br>
      <strong>Zhihang Zhong</strong>,
      Mingxi Cheng,
      Zhirong Wu,
      Yuhui Yuan,
      Yinqiang Zheng,
      Ji Li,
      Han Hu,
      Stephen Lin,
      Yoichi Sato,
      Imari Sato
      <br>
      <em>Under Review</em>, 2023
      <br>
      <a href="https://arxiv.org/abs/2211.11492" target="_blank">arXiv</a>
    </td>
  </tr> 
</tbody>
</table>

---
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=500&t=tt&d=XXbPPAPR_Tykk65fLeKabiB6-HTFXjsQRAiCOlmsK7w&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>