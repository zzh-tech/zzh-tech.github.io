---
permalink: /
excerpt: "About"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<br />

Hi, こんにちは, 你好 :-)

> Zhihang Zhong is actively looking for job opportunities!

He is a 3rd year Ph.D. student in the Department of Computer Science, the University of Tokyo (UTokyo).
His supervisor is <a href="https://scholar.google.com/citations?user=gtfbzYwAAAAJ&hl=en" target="_blank">Prof. Imari Sato</a>, and he works closely with <a href="https://scholar.google.com/citations?hl=en&user=JD-5DKcAAAAJ" target="_blank">Prof. Yinqiang Zheng</a> and <a href="https://sites.google.com/ut-vision.org/ysato/" target="_blank">Prof. Yoichi Sato</a>.
In 2020, he received his M.E. degree from UTokyo under the supervision of <a href="https://otalab.race.t.u-tokyo.ac.jp/en/jun-ota/" target="_blank">Prof. Ota Jun</a>.
In 2018, he received his B.E. degree (Mixed Honors Class) from Chu Kochen Honors College, Zhejiang University. 

Below are his current and previous internship experiences:  
- 2023.03 - present: Research intern at Computational Imaging Team, Snap Research, mentored by <a href="https://jianwang-cmu.github.io/" target="_blank">Jian Wang</a>, <a href="https://sizhuoma.netlify.app/" target="_blank">Sizhuo Ma</a> and <a href="http://www.cs.columbia.edu/~nayar/" target="_blank">Shree K. Nayar</a> 
- 2022.04 - 2022.12: JEM intern at Visual Computing group, Microsoft Research, mentored by <a href="https://scholar.google.com/citations?user=Jkss014AAAAJ&hl=en" target="_blank">Han Hu</a>, <a href="https://scholar.google.com/citations?hl=en&user=PzyvzksAAAAJ" target="_blank">Yuhui Yuan</a> and <a href="https://scholar.google.com/citations?hl=en&user=xyc52moAAAAJ" target="_blank">Ji Li</a>
- 2021.09 - 2022.03: D-CORE intern at Visual Computing group, Microsoft Research Asia (MSRA), mentored by <a href="https://scholar.google.com/citations?hl=en&user=c3PYmxUAAAAJ" target="_blank">Stephen Lin</a>, <a href="https://scholar.google.com/citations?hl=en&user=lH4zgcIAAAAJ" target="_blank">Zhirong Wu</a> and <a href="https://scholar.google.com/citations?hl=en&user=wYIe0tYAAAAJ" target="_blank">Xiao Sun</a>

Zhihang's current research interests include computational photography, computer vision, machine/deep learning, and human-computer interaction.

---

News
======

<details>
    <summary>[2023.02] Two papers are accepted by CVPR 2023.</summary>
    [2022.10] I give a talk at <a
            href="https://mipi-challenge.org/#:~:text=Presenter%3A%20Zhihang%20Zhong%20(The%20University%20of%20Tokyo)"
            target="_blank">MIPI Workshop 2022</a>. <br>
    [2022.10] One paper is accepted by IJCV. <br>
    [2022.09] I become a JSPS「日本学術振興会」DC fellow! <br>
    [2022.07] Three papers (one Oral) are accepted by ECCV 2022! <br>
    [2022.04] I become a JEM intern at Microsoft. <br>
    [2022.03] One paper is accepted by CVPR 2022. <br>
    [2021.09] I become a research intern in the Visual Computing group at MSRA. <br>
    [2021.04] I become a IIW fellow「知能社会創造フェローシップ生」of UTokyo! <br>
    [2021.04] One paper is accepted by IoTJ. <br>
    [2021.02] One paper is accepted by CVPR 2021. <br>
    [2020.11] I become a MSRA D-CORE fellow! <br>
    [2020.09] I obtain my M.E. degree from UTokyo with an outstanding thesis award! <br>
    [2020.07] One paper (Spotlight) is accepted by ECCV 2020! <br>
    [2019.12] One paper is accepted by IUI 2020. <br>
</details>

---

# Publications
<head>
    <style>
    table,
    th,
    td {
        border: 0px solid darkgray;
    }
    </style><title></title>
</head>

## Conferences

<table style="width:100%;border:0px;border-spacing:0px;margin-right:auto;margin-left:auto;">
<tbody>
  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/bit++.gif' width="180">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Blur Interpolation Transformer for Real-World Motion from Blur</b>
      <br>
      <strong>Zhihang Zhong</strong>,
      Mingdeng Cao,
      Xiang Ji,
      Yinqiang Zheng,
      Imari Sato
      <br>
      <em>CVPR</em>, 2023
      <br>
      <a href="https://zzh-tech.github.io/BiT/" target="_blank">project page</a> /
      <a href="https://arxiv.org/abs/2211.11423" target="_blank">arXiv</a> /
      <a href="https://github.com/zzh-tech/BiT" target="_blank">code</a>
    </td>
  </tr>
  
  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/VIS-WIDE.png' width="160">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Visibility Constrained Wide-band Illumination Spectrum Design for Seeing-in-the-Dark</b>
      <br>
      Muyao Niu,
      Zhuoxiao Li,
      <strong>Zhihang Zhong</strong>,
      Yinqiang Zheng
      <br>
      <em>CVPR</em>, 2023
      <br>
      arXiv
    </td>
  </tr>  
  
  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
      <img src="images/animation-from-blur.gif" width="130">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Animation from Blur: Multi-modal Blur Decomposition with Motion Guidance</b>
      <br>
      <strong>Zhihang Zhong</strong>,
      Xiao Sun, 
      Zhirong Wu,
      Yinqiang Zheng,
      Stephen Lin,
      Imari Sato
      <br>
      <em>ECCV</em>, 2022
      <br>
      <a href="https://zzh-tech.github.io/Animation-from-Blur/" target="_blank">project page</a> /
      <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7210_ECCV_2022_paper.php" target="_blank">paper</a> /
      <a href="https://arxiv.org/abs/2207.10123" target="_blank">arXiv</a> / 
      <a href="https://github.com/zzh-tech/Animation-from-Blur" target="_blank">code</a>
    </td>
  </tr> 

  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/dual-reversed-rs.gif' width="130">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Bringing Rolling Shutter Images Alive with Dual Reversed Distortion</b>
      <br>
      <strong>Zhihang Zhong</strong>,
       Mingdeng Cao,
       Xiao Sun,
       Zhirong Wu,
       Zhongyi Zhou,
       Yinqiang Zheng,
       Stephen Lin,
       Imari Sato
      <br>
      <em>ECCV</em>, 2022, <em style="color: red">Oral (top 2.7%)</em>
      <br>
      <a href="https://zzh-tech.github.io/Dual-Reversed-RS/" target="_blank">project page</a> /
      <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4547_ECCV_2022_paper.php" target="_blank">paper</a> /
      <a href="https://arxiv.org/abs/2203.06451" target="_blank">arXiv</a> / 
      <a href="https://github.com/zzh-tech/Dual-Reversed-RS" target="_blank">code</a>
    </td>
  </tr> 
    
  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/magnitude_prior.png' width="200">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Efficient Video Deblurring Guided by Motion Magnitude</b>
      <br>
      Yusheng Wang, 
      Yunfan Lu, 
      Ye Gao, 
      Lin Wang, 
      <strong>Zhihang Zhong</strong>, 
      Yinqiang Zheng, 
      Atsushi Yamashita
      <br>
      <em>ECCV</em>, 2022
      <br>
      <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5697_ECCV_2022_paper.php" target="_blank">paper</a> / 
      <a href="https://arxiv.org/abs/2207.13374" target="_blank">arXiv</a> / 
      <a href="https://github.com/sollynoay/MMP-RNN" target="_blank">code</a>
    </td>
  </tr>

  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/adaptive_warping.png' width="180">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Learning Adaptive Warping for Real-World Rolling Shutter Correction</b>
      <br>
      Mingdeng Cao, 
      <strong>Zhihang Zhong</strong>, 
      Jiahao Wang, 
      Yinqiang Zheng, 
      Yujiu Yang
      <br>
      <em>CVPR</em>, 2022
      <br>
      <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cao_Learning_Adaptive_Warping_for_Real-World_Rolling_Shutter_Correction_CVPR_2022_paper.html" target="_blank">paper</a> / 
      <a href="https://arxiv.org/abs/2204.13886" target="_blank">arXiv</a> / 
      <a href="https://github.com/ljzycmd/BSRSC" target="_blank">code</a>
    </td>
  </tr>  

  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/beam_splitter_a.png' width="150">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Towards Rolling Shutter Correction and Deblurring in Dynamic Scenes</b>
      <br>
      <strong>Zhihang Zhong</strong>,
      Yinqiang Zheng, 
      Imari Sato
      <br>
      <em>CVPR</em>, 2021
      <br>
      <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhong_Towards_Rolling_Shutter_Correction_and_Deblurring_in_Dynamic_Scenes_CVPR_2021_paper.html" target="_blank">paper</a> / 
      <a href="https://arxiv.org/abs/2104.01601" target="_blank">arXiv</a> / 
      <a href="https://github.com/zzh-tech/RSCD" target="_blank">code</a>
    </td>
  </tr>  

  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/2020_ECCV_deblur.png' width="220">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Efficient Spatio-Temporal Recurrent Neural Network for Video Deblurring</b>
      <br>
      <strong>Zhihang Zhong</strong>,
      Ye Gao,
      Yinqiang Zheng,
      Bo Zheng
      <br>
      <em>ECCV</em>, 2020, <em style="color: red">Spotlight (top 5.0%)</em>
      <br>
      <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5116_ECCV_2020_paper.php" target="_blank">paper</a> / 
      <a href="https://arxiv.org/abs/2106.16028" target="_blank">arXiv</a> / 
      <a href="https://github.com/zzh-tech/ESTRNN" target="_blank">code</a>
    </td>
  </tr>  

  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/layer.png' width="220">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Multi-attention deep recurrent neural network for nursing action evaluation using wearable sensor</b>
      <br>
      <strong>Zhihang Zhong</strong>,
      Chingszu Lin,
      Taiki Ogata,
      Jun Ota
      <br>
      <em>IUI</em>, 2020
      <br>
      <a href="https://dl.acm.org/doi/abs/10.1145/3377325.3377530" target="_blank">paper</a>
    </td>
  </tr>  
</tbody>
</table>

## Journals
<table style="width:100%;border:0px;border-spacing:0px;margin-right:auto;margin-left:auto;">
<tbody>
    <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/bsd.png' width="200">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Real-world Video Deblurring: A Benchmark Dataset and An Eﬃcient Recurrent Neural Network</b>
      <br>
      <strong>Zhihang Zhong</strong>,
      Ye Gao,
      Yinqiang Zheng,
      Bo Zheng,
      Imari Sato
      <br>
      <em>International Journal of Computer Vision (IJCV)</em>, 2022  
      <br>
      <a href="https://link.springer.com/article/10.1007/s11263-022-01705-6" target="_blank">paper</a> / 
      <a href="https://arxiv.org/abs/2106.16028" target="_blank">arXiv</a> / 
      <a href="https://github.com/zzh-tech/ESTRNN" target="_blank">code</a>
    </td>
  </tr>  
  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/zhong10-3075477-large.jpg' width="130">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Multistream Temporal Convolutional Network for Correct/Incorrect Patient Transfer Action Detection Using Body Sensor Network</b>
      <br>
      <strong>Zhihang Zhong</strong>,
      Chingszu Lin,
      Masako Kanai-Pak,
      Jukai Maeda,
      Yasuko Kitajima,
      Mitsuhiro Nakamura,
      Noriaki Kuwahara,
      Taiki Ogata,
      Jun Ota
      <br>
      <em>IEEE Internet of Things Journal (IoTJ)</em>, 2021  
      <br>
      <a href="https://ieeexplore.ieee.org/document/9415629" target="_blank">paper</a> / 
      <a href="https://github.com/zzh-tech/Continuous-Action-Detection" target="_blank">code</a>  
    </td>
  </tr>  
  <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/patient_robot.png' width="240">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>Development and validation of robot patient equipped with an inertial measurement unit and angular position sensors to evaluate transfer skills of nurses</b>
      <br>
      Chingszu Lin,
      Taiki Ogata,
      <strong>Zhihang Zhong</strong>,
      Masako Kanai-Pak,
      Jukai Maeda,
      Yasuko Kitajima,
      Mitsuhiro Nakamura,
      Noriaki Kuwahara,
      Jun Ota
      <br>
      <em>International Journal of Social Robotics (IJSR)</em>, 2021  
      <br>
      <a href="https://link.springer.com/article/10.1007/s12369-020-00673-6" target="_blank">paper</a>
    </td>
  </tr>
</tbody>
</table>

## Preprints
<table style="width:100%;border:0px;border-spacing:0px;margin-right:auto;margin-left:auto;">
<tbody>
    <tr onmouseout="sfp_stop()" onmouseover="sfp_start()">
    <td style="padding:20px;width:30%;vertical-align:middle;text-align: center">
        <img src='images/clipcrop.png' width="200">
    </td>
    <td style="padding:20px;width:70%;vertical-align:middle">
        <b>ClipCrop: Conditioned Cropping Driven by Vision-Language Model</b>
      <br>
      <strong>Zhihang Zhong</strong>,
      Mingxi Cheng,
      Zhirong Wu,
      Yuhui Yuan,
      Yinqiang Zheng,
      Ji Li,
      Han Hu,
      Stephen Lin,
      Yoichi Sato,
      Imari Sato
      <br>
      <em>Under Review</em>, 2023
      <br>
      <a href="https://arxiv.org/abs/2211.11492" target="_blank">arXiv</a>
    </td>
  </tr> 
</tbody>
</table>

---
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=500&t=tt&d=XXbPPAPR_Tykk65fLeKabiB6-HTFXjsQRAiCOlmsK7w&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>